{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I will train your CNN-RNN model.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, I will customize the training of my CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values I set now will be used when training my model in **Step 2** below.\n",
    "\n",
    "\n",
    "### Task #1\n",
    "\n",
    "I begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "\n",
    "I have implemented a modified version of the paper \"Show and Tell: A Neural Image Captioning Generator\". The model is basically the same that is presented in the paper. So, there is an encoder that is a CNN pretrained model - ResNet50 with the final classifier section removed instead with a linear layer at the end, and a decoder that is an LSTM. The main difference is that my model does not apply softmax to the output of the decoder because this caused a deterioration in performance. Both hidden_size and embed_size are setted to 512 as in the paper. I have tested three values of batch_size, 64-128-256, and I have obtained best results with 64.\n",
    "\n",
    "\n",
    "Pre-trained ResNet model is being used for this project as a CNN encoder. So, the initial layers of the CNN are already well-trained. We are only modifying the last part of the CNN to provide the feature vector we need. So, it made sense to train only the embedding layer parameters of the CNN.\n",
    "\n",
    "For the RNN decoder, no layer is previously trained. So, it made sense to train parameters of all layers of decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.2.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:31<00:00, 4546.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.86s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 64          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "#3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "#4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/6471], Loss: 3.6525, Perplexity: 38.569238\n",
      "Epoch [1/1], Step [200/6471], Loss: 3.3973, Perplexity: 29.8843\n",
      "Epoch [1/1], Step [300/6471], Loss: 3.3941, Perplexity: 29.78657\n",
      "Epoch [1/1], Step [400/6471], Loss: 3.1025, Perplexity: 22.2525\n",
      "Epoch [1/1], Step [500/6471], Loss: 3.4784, Perplexity: 32.40764\n",
      "Epoch [1/1], Step [600/6471], Loss: 3.5581, Perplexity: 35.0959\n",
      "Epoch [1/1], Step [700/6471], Loss: 2.8689, Perplexity: 17.61857\n",
      "Epoch [1/1], Step [800/6471], Loss: 3.3553, Perplexity: 28.6538\n",
      "Epoch [1/1], Step [900/6471], Loss: 2.7444, Perplexity: 15.5548\n",
      "Epoch [1/1], Step [1000/6471], Loss: 3.0181, Perplexity: 20.4525\n",
      "Epoch [1/1], Step [1100/6471], Loss: 2.6536, Perplexity: 14.2047\n",
      "Epoch [1/1], Step [1200/6471], Loss: 2.7702, Perplexity: 15.9624\n",
      "Epoch [1/1], Step [1300/6471], Loss: 2.7489, Perplexity: 15.62604\n",
      "Epoch [1/1], Step [1400/6471], Loss: 2.6007, Perplexity: 13.4728\n",
      "Epoch [1/1], Step [1500/6471], Loss: 2.7508, Perplexity: 15.6557\n",
      "Epoch [1/1], Step [1600/6471], Loss: 2.8651, Perplexity: 17.5514\n",
      "Epoch [1/1], Step [1700/6471], Loss: 2.6238, Perplexity: 13.7877\n",
      "Epoch [1/1], Step [1800/6471], Loss: 2.3821, Perplexity: 10.8278\n",
      "Epoch [1/1], Step [1900/6471], Loss: 2.6889, Perplexity: 14.7156\n",
      "Epoch [1/1], Step [2000/6471], Loss: 2.6486, Perplexity: 14.1345\n",
      "Epoch [1/1], Step [2100/6471], Loss: 2.4997, Perplexity: 12.1783\n",
      "Epoch [1/1], Step [2200/6471], Loss: 2.4307, Perplexity: 11.3671\n",
      "Epoch [1/1], Step [2300/6471], Loss: 2.5394, Perplexity: 12.6718\n",
      "Epoch [1/1], Step [2400/6471], Loss: 2.6823, Perplexity: 14.6192\n",
      "Epoch [1/1], Step [2500/6471], Loss: 2.3139, Perplexity: 10.1142\n",
      "Epoch [1/1], Step [2600/6471], Loss: 2.4114, Perplexity: 11.14953\n",
      "Epoch [1/1], Step [2700/6471], Loss: 2.6051, Perplexity: 13.5324\n",
      "Epoch [1/1], Step [2800/6471], Loss: 2.3114, Perplexity: 10.0888\n",
      "Epoch [1/1], Step [2900/6471], Loss: 2.4470, Perplexity: 11.5539\n",
      "Epoch [1/1], Step [3000/6471], Loss: 2.1198, Perplexity: 8.32941\n",
      "Epoch [1/1], Step [3100/6471], Loss: 2.5910, Perplexity: 13.3429\n",
      "Epoch [1/1], Step [3200/6471], Loss: 2.1443, Perplexity: 8.53623\n",
      "Epoch [1/1], Step [3300/6471], Loss: 2.2825, Perplexity: 9.80071\n",
      "Epoch [1/1], Step [3500/6471], Loss: 2.4313, Perplexity: 11.3733\n",
      "Epoch [1/1], Step [3600/6471], Loss: 2.1877, Perplexity: 8.91470\n",
      "Epoch [1/1], Step [3700/6471], Loss: 2.2490, Perplexity: 9.47796\n",
      "Epoch [1/1], Step [3800/6471], Loss: 2.3289, Perplexity: 10.2668\n",
      "Epoch [1/1], Step [3900/6471], Loss: 2.3902, Perplexity: 10.9156\n",
      "Epoch [1/1], Step [4000/6471], Loss: 2.5474, Perplexity: 12.77440\n",
      "Epoch [1/1], Step [4100/6471], Loss: 2.3486, Perplexity: 10.4711\n",
      "Epoch [1/1], Step [4200/6471], Loss: 2.1580, Perplexity: 8.65420\n",
      "Epoch [1/1], Step [4300/6471], Loss: 2.1297, Perplexity: 8.41209\n",
      "Epoch [1/1], Step [4400/6471], Loss: 2.3708, Perplexity: 10.7062\n",
      "Epoch [1/1], Step [4500/6471], Loss: 2.2099, Perplexity: 9.11502\n",
      "Epoch [1/1], Step [4600/6471], Loss: 2.6570, Perplexity: 14.2538\n",
      "Epoch [1/1], Step [4700/6471], Loss: 2.2487, Perplexity: 9.47551\n",
      "Epoch [1/1], Step [4800/6471], Loss: 2.3601, Perplexity: 10.5918\n",
      "Epoch [1/1], Step [4900/6471], Loss: 2.2531, Perplexity: 9.51754\n",
      "Epoch [1/1], Step [5000/6471], Loss: 2.2619, Perplexity: 9.60154\n",
      "Epoch [1/1], Step [5100/6471], Loss: 2.2565, Perplexity: 9.54929\n",
      "Epoch [1/1], Step [5200/6471], Loss: 2.1728, Perplexity: 8.78256\n",
      "Epoch [1/1], Step [5300/6471], Loss: 2.1295, Perplexity: 8.41051\n",
      "Epoch [1/1], Step [5400/6471], Loss: 2.3329, Perplexity: 10.3076\n",
      "Epoch [1/1], Step [5500/6471], Loss: 2.3786, Perplexity: 10.7900\n",
      "Epoch [1/1], Step [5600/6471], Loss: 2.4450, Perplexity: 11.5301\n",
      "Epoch [1/1], Step [5700/6471], Loss: 3.0229, Perplexity: 20.5517\n",
      "Epoch [1/1], Step [5800/6471], Loss: 2.5413, Perplexity: 12.6967\n",
      "Epoch [1/1], Step [5900/6471], Loss: 2.3674, Perplexity: 10.6691\n",
      "Epoch [1/1], Step [6000/6471], Loss: 2.1856, Perplexity: 8.89573\n",
      "Epoch [1/1], Step [6100/6471], Loss: 2.3183, Perplexity: 10.1584\n",
      "Epoch [1/1], Step [6200/6471], Loss: 2.5407, Perplexity: 12.6882\n",
      "Epoch [1/1], Step [6300/6471], Loss: 2.1287, Perplexity: 8.40389\n",
      "Epoch [1/1], Step [6400/6471], Loss: 2.3587, Perplexity: 10.5770\n",
      "Epoch [1/1], Step [6471/6471], Loss: 2.5456, Perplexity: 12.7505"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
